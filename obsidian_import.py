#!/usr/bin/env python3

# ğŸ“ Obsidian å¯¼å…¥éœ€æ±‚:
# (Copilot å¿…é¡»åœ¨ç¼–è¾‘ä»£ç æ—¶åŒæ­¥è¿™äº›éœ€æ±‚ï¼Œå¦‚æœéœ€æ±‚å‘ç”Ÿå˜åŒ–ï¼)
#
# æ­¤è„šæœ¬å¤„ç†ä» Notion å¯¼å‡ºçš„ç›®å½•ï¼Œå…¶ä¸­åŒ…å«å¤šä¸ª Markdown æ–‡ä»¶
# ï¼ˆæ–‡ä»¶åæ ¼å¼ä¸ºï¼š"DocumentName UID.md"ï¼‰ã€‚å¦‚æœ Markdown æ–‡ä»¶å¼•ç”¨äº†é™„ä»¶ï¼Œ
# å®ƒä»¬å­˜å‚¨åœ¨ä¸€ä¸ªå•ç‹¬çš„ç›®å½•ä¸­ï¼ˆç›®å½•åç§°ä¸ Markdown æ–‡ä»¶ç›¸åŒï¼‰ã€‚å¯èƒ½æœ‰å¤šä¸ªé™„ä»¶ã€‚
# å¦‚æœæ–‡æ¡£åç§°ç›¸åŒï¼Œåˆ™ä½¿ç”¨ UID æ¥åŒºåˆ†å®ƒä»¬ã€‚
#
# ğŸ¯ ç›®æ ‡:
# - ğŸ“„ ä» Markdown æ–‡ä»¶åä¸­ç§»é™¤ UIDï¼ˆå¦‚æœæ²¡æœ‰é‡å¤æ–‡ä»¶ï¼‰ã€‚
# - ğŸ—‚ï¸ å°†æ‰€æœ‰é™„ä»¶æ•´åˆåˆ°ä¸€ä¸ª "Resource" ç›®å½•ä¸­ã€‚
# - âœï¸ æ ¹æ® Markdown æ–‡ä»¶åé‡å‘½åé™„ä»¶ã€‚å¦‚æœæœ‰å¤šä¸ªé™„ä»¶ï¼Œ
#   åœ¨åç§°åé™„åŠ åºåˆ—å·ã€‚
# - ğŸ–¼ï¸ å¦‚æœ Markdown æ–‡ä»¶ä»…å¼•ç”¨ä¸€ä¸ªå›¾ç‰‡ï¼Œåˆ™ä¿æŒå›¾ç‰‡åç§°ä¸æ–‡æ¡£åç§°ç›¸åŒï¼Œ
#   ä¸æ·»åŠ åºåˆ—å·ã€‚
# - ğŸ”„ æ›´æ–° Markdown æ–‡ä»¶ä¸­çš„å¼•ç”¨ä»¥åæ˜ æ–°çš„é™„ä»¶è·¯å¾„ã€‚
# - ğŸ·ï¸ æ ¹æ®ç”¨æˆ·å®šä¹‰çš„è§„åˆ™å¤„ç†å¹¶è½¬æ¢ Markdown æ–‡ä»¶ä¸­çš„å…ƒæ•°æ®ã€‚
#   - âœ… æ ¹æ®é…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„è§„åˆ™éªŒè¯å…ƒæ•°æ®ã€‚
#   - ğŸ› ï¸ ä¿®æ”¹å…ƒæ•°æ®é”®æˆ–å€¼ï¼Œè¿½åŠ é¢å¤–å†…å®¹ï¼Œæˆ–åˆ é™¤å…ƒæ•°æ®æ¡ç›®ã€‚
#   - ğŸ“‹ è®°å½•æœªæ˜ å°„çš„å…ƒæ•°æ®ä»¥ä¾›å®¡æŸ¥ã€‚
# - ğŸ§¹ æ¸…ç†å·²å¤„ç†çš„ Markdown æ–‡ä»¶åŠå…¶å¯¹åº”çš„é™„ä»¶ç›®å½•ã€‚
#
# ğŸ“‹ æ­¥éª¤:
# 1) ğŸ·ï¸ å¤„ç† Markdown æ–‡ä»¶ä¸­çš„å…ƒæ•°æ®ï¼š
#    - âœ… æ ¹æ®ç”¨æˆ·å®šä¹‰çš„è§„åˆ™éªŒè¯å…ƒæ•°æ®ã€‚
#    - ğŸ› ï¸ æ ¹æ®è§„åˆ™è½¬æ¢å…ƒæ•°æ®é”®æˆ–å€¼ã€‚
#    - ğŸ“‹ åœ¨ `unmapped_metadata` å­—å…¸ä¸­è®°å½•æœªæ˜ å°„çš„å…ƒæ•°æ®ä»¥ä¾›å®¡æŸ¥ã€‚
# 2) ğŸ” æœç´¢ Markdown æ–‡ä»¶ä¸­å¼•ç”¨çš„é™„ä»¶å¹¶å°†å…¶ç§»åŠ¨åˆ° "Resource" ç›®å½•ï¼š
#    - ğŸ–¼ï¸ å¦‚æœåªæœ‰ä¸€ä¸ªé™„ä»¶ï¼Œå°†å…¶é‡å‘½åä¸ºä¸ Markdown æ–‡ä»¶åç›¸åŒä¸”ä¸å¸¦åºåˆ—å·ã€‚
#    - ğŸ“š å¦‚æœæœ‰å¤šä¸ªé™„ä»¶ï¼Œåœ¨åç§°åé™„åŠ åºåˆ—å·ã€‚
#    - ğŸ”„ ç»´æŠ¤æ—§é™„ä»¶è·¯å¾„åˆ°æ–°è·¯å¾„çš„æ˜ å°„ (`path_mapping`) ä»¥æ›´æ–°å¼•ç”¨ã€‚
#    - ğŸ—‘ï¸ å¦‚æœé™„ä»¶ç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡ä¸ Markdown æ–‡ä»¶ä¸­çš„å¼•ç”¨æ•°é‡ä¸€è‡´ï¼Œ
#      åˆ›å»ºæ¸…ç†è¯¥é™„ä»¶ç›®å½•çš„ä»»åŠ¡ã€‚
# 3) ğŸ”„ æ›´æ–° Markdown æ–‡ä»¶ä»¥åæ˜ æ–°çš„é™„ä»¶è·¯å¾„ï¼š
#    - ğŸ“„ ä½¿ç”¨ `path_mapping` æ›¿æ¢ Markdown å†…å®¹ä¸­çš„æ—§è·¯å¾„ä¸ºæ–°è·¯å¾„ã€‚
#    - åŒ…æ‹¬è¡Œå·ã€åŸå§‹æ–‡æœ¬å’Œæ›´æ–°åçš„æ–‡æœ¬ä»¥æ›´æ–°é“¾æ¥ã€‚
# 4) âœï¸ é‡å‘½å Markdown æ–‡ä»¶ä»¥ç§»é™¤ UIDï¼ˆå¦‚æœæ²¡æœ‰é‡å¤æ–‡ä»¶ï¼‰ï¼š
#    - ğŸ“„ åœ¨å•ç‹¬çš„è¡Œä¸­æ˜¾ç¤ºæºè·¯å¾„å’Œç›®æ ‡è·¯å¾„ä»¥ä¾¿äºæ¯”è¾ƒã€‚
# 5) ğŸ§¹ æ¸…ç†å·²å¤„ç†çš„ Markdown æ–‡ä»¶åŠå…¶å¯¹åº”çš„é™„ä»¶ç›®å½•ï¼š
#    - ğŸ—‘ï¸ åœ¨å¤„ç†ååˆ é™¤åŸå§‹ Markdown æ–‡ä»¶åŠå…¶é™„ä»¶ç›®å½•ã€‚
# 6) ğŸ“Š æ‰«æåæ‰“å°æ€»ç»“ä»»åŠ¡çš„ç»Ÿè®¡ä¿¡æ¯ã€‚
# 7) â“ åœ¨æ‰§è¡Œä»»åŠ¡åˆ—è¡¨ä¹‹å‰æç¤ºç”¨æˆ·ç¡®è®¤ã€‚
# 8) ğŸ”„ å¦‚æœåœ¨æ‰«ææœŸé—´æ£€æµ‹åˆ°é‡å¤åç§°ï¼Œé™„åŠ åºåˆ—å·ä»¥è§£å†³å†²çªã€‚
#    ç¡®ä¿ Markdown æ–‡ä»¶ä¸å…¶é™„ä»¶ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚
# 9) ğŸ“ åœ¨æ‰§è¡Œä»»åŠ¡åç”Ÿæˆæ€»ç»“æŠ¥å‘Šã€‚

# ğŸ·ï¸ å…³äºå…ƒæ•°æ®çš„è¯´æ˜:
# å…ƒæ•°æ®æ˜¯ Markdown æ–‡ä»¶ä¸­çš„å­—å…¸æ•°æ®ï¼Œæ ¼å¼ä¸º "meta name: meta value"ã€‚
# æ¯ä¸ªå…ƒæ•°æ®å ä¸€è¡Œï¼Œé€šå¸¸å‡ºç°åœ¨æ–‡ä»¶çš„å‰åè¡Œå†…ã€‚
# å¤šä¸ªå…ƒæ•°æ®æ¡ç›®ä¹‹é—´æ²¡æœ‰ç©ºè¡Œã€‚
# å…ƒæ•°æ®éƒ¨åˆ†é€šå¸¸é€šè¿‡ç©ºè¡Œæˆ– "---" è¡Œä¸å†…å®¹çš„å…¶ä½™éƒ¨åˆ†åˆ†éš”ã€‚

# å…ƒæ•°æ®å¤„ç†ç±»å‹å’Œæ“ä½œ:
# 1) "retain" - ä¿ç•™å…ƒæ•°æ®ï¼Œä¸åšä»»ä½•æ›´æ”¹ã€‚
#    - å¦‚æœ "retain" æ˜¯å”¯ä¸€çš„æ“ä½œï¼Œåˆ™å…ƒæ•°æ®å°†è¢«è·³è¿‡ï¼Œä¸åšä»»ä½•æ›´æ”¹ã€‚
#    - å¦‚æœåŒæ—¶å­˜åœ¨ "retain" å’Œ "delete"ï¼Œ"delete" ä¼˜å…ˆï¼Œå…ƒæ•°æ®å°†è¢«åˆ é™¤ã€‚
# 2) "delete" - å®Œå…¨åˆ é™¤å…ƒæ•°æ®æ¡ç›®ã€‚
#    - æ­¤æ“ä½œä¼˜å…ˆçº§æœ€é«˜ï¼Œå¦‚æœå­˜åœ¨ï¼Œå°†è¦†ç›–å…¶ä»–æ“ä½œã€‚
# 3) "rename" - æ›´æ”¹å…ƒæ•°æ®é”®åï¼ŒåŒæ—¶ä¿ç•™å€¼ã€‚
# 4) "modify_value" - ä½¿ç”¨ç›´æ¥æ˜ å°„æˆ–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼è½¬æ¢å…ƒæ•°æ®å€¼ã€‚
#    - ç›´æ¥æ˜ å°„ï¼šç”¨é¢„å®šä¹‰çš„æ˜ å°„æ›¿æ¢ç‰¹å®šå€¼ã€‚
#    - æ­£åˆ™æ˜ å°„ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¹¶æ›¿æ¢å€¼ã€‚
#    - "modify_value" åœ¨ "append_after" å’Œ "rename" ä¹‹å‰åº”ç”¨ã€‚
# 5) "append_after" - åœ¨ç°æœ‰å…ƒæ•°æ®å€¼åæ·»åŠ æ–‡æœ¬ã€‚
#
# æœªæ˜ å°„çš„å…ƒæ•°æ®:
# - ä¸ç¬¦åˆé…ç½®ä¸­ä»»ä½•è§„åˆ™çš„å…ƒæ•°æ®å€¼å°†è¢«è®°å½•ä»¥ä¾›å®¡æŸ¥ã€‚
# - è¿™äº›æœªæ˜ å°„çš„å€¼å­˜å‚¨åœ¨å­—å…¸ (`unmapped_metadata`) ä¸­ä»¥ä¾›è¿›ä¸€æ­¥åˆ†æã€‚

import os
import re
import shutil
import argparse
import sys
from enum import Enum
from pathlib import Path
import time
from urllib.parse import quote, unquote
import yaml
import datetime
import logging
import typing
from typing import List, Dict, Tuple, Optional, Union, Any
import uuid  # Add this import for generating unique IDs

# Removed duplicate `shutil` import and unused `defaultdict` import
# Fixed import order to comply with PEP8

#############################################################
# LOGGING FUNCTION
#############################################################

# Define log level constants first
LOG_LEVEL_ERROR = "ERR"
LOG_LEVEL_FLOW = "FLW"
LOG_LEVEL_ACTION = "ACT"
LOG_LEVEL_DEBUG = "DBG"

class TaskType(Enum):
    """Enum defining different types of tasks that can be performed during the import process."""
    RENAME_MD = "RENAME_MD"
    MOVE_ATTACHMENT = "MOVE_ATTACHMENT"
    COPY_ATTACHMENT = "COPY_ATTACHMENT"
    UPDATE_ATTACH_REF = "UPDATE_ATTACH_REF"
    TRANSFORM_METADATA = "TRANSFORM_METADATA"
    MAP_METADATA = "MAP_METADATA"
    CLEANUP = "CLEANUP"

# å®šä¹‰æ—¥å¿—çº§åˆ«
LOG_LEVELS = {
    LOG_LEVEL_ERROR: logging.ERROR,       # å¼‚å¸¸ï¼šæµç¨‹ä¸­é€»è¾‘å†²çªå’Œç¨‹åºå¼‚å¸¸
    LOG_LEVEL_FLOW: 25,                   # æµç¨‹ï¼šå‡½æ•°å…¥å£ç‚¹å’Œæ­¥éª¤ç±»å‡½æ•°
    LOG_LEVEL_ACTION: 15,                 # åŠ¨ä½œï¼šæ·»åŠ ä»»åŠ¡ï¼Œæ‰§è¡ŒåŠ¨ä½œç­‰å®è´¨æ€§åŠ¨ä½œ
    LOG_LEVEL_DEBUG: logging.DEBUG,       # è°ƒè¯•ï¼šè¯¦ç»†è°ƒè¯•ä¿¡æ¯
}

# æ³¨å†Œè‡ªå®šä¹‰æ—¥å¿—çº§åˆ«
logging.addLevelName(25, "ACTION")
logging.addLevelName(15, "FLOW")

def debug(message: str, level: str, config: Dict[str, Any]) -> None:
    """
    ç»Ÿä¸€çš„è°ƒè¯•å’Œæ—¥å¿—è®°å½•å‡½æ•°ã€‚

    å‚æ•°:
        message (str): è¦è®°å½•çš„æ¶ˆæ¯
        level (str): æ¶ˆæ¯çš„çº§åˆ«ï¼ˆerror, action, flow, debugï¼‰
        config (dict): é…ç½®å­—å…¸ï¼Œæ§åˆ¶æ—¥å¿—å’Œæ ‡å‡†è¾“å‡ºçš„çº§åˆ«
    """
    if config is None:
        config = {}

    # è·å–æ—¥å¿—å’Œæ ‡å‡†è¾“å‡ºçš„çº§åˆ«
    log_level = LOG_LEVELS.get(config.get("log_level", LOG_LEVEL_ACTION), logging.NOTSET)
    stdout_level = LOG_LEVELS.get(config.get("stdout_level", LOG_LEVEL_FLOW), logging.NOTSET)

    # ä¸ºä¸åŒæ—¥å¿—çº§åˆ«å®šä¹‰emoji
    level_emojis = {
        LOG_LEVEL_ERROR: "âŒ ",
        LOG_LEVEL_ACTION: "âš¡ ",
        LOG_LEVEL_FLOW: "ğŸ‘£ ",
        LOG_LEVEL_DEBUG: "ğŸ "
    }

    # æ„å»ºæ—¥å¿—æ¶ˆæ¯
    log_message = f"{level_emojis.get(level, '')}{message}"

    # æ‰“å°åˆ°æ ‡å‡†è¾“å‡º
    if LOG_LEVELS[level] >= stdout_level:
        print(log_message)

    # å†™å…¥æ—¥å¿—æ–‡ä»¶
    log_file_handle = config.get("log_file_handle")
    if log_file_handle and LOG_LEVELS[level] >= log_level:
        try:
            log_file_handle.write(f"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {log_message}\n")
            log_file_handle.flush()  # ç¡®ä¿æ—¥å¿—ç«‹å³å†™å…¥æ–‡ä»¶
        except Exception as e:
            print(f"Error writing to log file: {e}")

    # å¦‚æœæ˜¯é”™è¯¯çº§åˆ«æ—¥å¿—ï¼Œå¢åŠ é”™è¯¯è®¡æ•°
    if level == LOG_LEVEL_ERROR and "stats" in config:
        config["stats"]["errors"] += 1

def probe_path(path: Union[str, Path], config: Dict[str, Any]) -> str:
    """
    æ¢æµ‹è·¯å¾„ï¼Œåˆ¤æ–­æ˜¯ç›®å½•ã€æ–‡ä»¶è¿˜æ˜¯ä¸å­˜åœ¨ã€‚

    å‚æ•°:
        path (str æˆ– Path): è¦æ¢æµ‹çš„è·¯å¾„
        config (Dict[str, Any], optional): é…ç½®å­—å…¸ï¼Œç”¨äºè®°å½•é”™è¯¯

    è¿”å›:
        str: è·¯å¾„ç±»å‹æè¿° - "directory"ã€"file"ã€"not_exist" æˆ– "error"
    """
    try:
        path_obj = Path(path) if isinstance(path, str) else path
        
        if not path_obj.exists():
            return "not_exist"
        elif path_obj.is_dir():
            return "directory"
        elif path_obj.is_file():
            return "file"
        else:
            # å¯èƒ½æ˜¯ç¬¦å·é“¾æ¥æˆ–å…¶ä»–ç‰¹æ®Šæ–‡ä»¶
            return "other"
    except Exception as e:
        if config is not None:
            debug(f"Error probing path {path}: {e}", LOG_LEVEL_ERROR, config)
        return "error"
    
def safe_open_file(file_path: str, mode: str, encoding: str = "utf-8") -> Optional[typing.IO]:
    """
    å®‰å…¨åœ°æ‰“å¼€æ–‡ä»¶ï¼Œå¤„ç†å¯èƒ½çš„å¼‚å¸¸ã€‚

    å‚æ•°:
        file_path (str): æ–‡ä»¶è·¯å¾„
        mode (str): æ‰“å¼€æ¨¡å¼
        encoding (str): æ–‡ä»¶ç¼–ç 

    è¿”å›:
        file object æˆ– None: å¦‚æœæ‰“å¼€å¤±è´¥è¿”å› None
    """
    try:
        return open(file_path, mode, encoding=encoding)
    except Exception as e:
        print(f"âŒ Error opening file {file_path}: {e}")
        return None

def safe_close_file(file_handle: Optional[typing.IO]) -> None:
    """
    å®‰å…¨åœ°å…³é—­æ–‡ä»¶å¥æŸ„ï¼Œå¤„ç†å¯èƒ½çš„å¼‚å¸¸ã€‚

    å‚æ•°:
        file_handle (file object): è¦å…³é—­çš„æ–‡ä»¶å¥æŸ„
    """
    try:
        if file_handle:
            file_handle.close()
            print(f"âœ… File closed successfully.")
    except Exception as e:
        print(f"âŒ Error closing file: {e}")

def initialize_log_file(config: Dict[str, Any]) -> None:
    """
    åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶ã€‚å¦‚æœå¯ç”¨äº† --reset å‚æ•°ï¼Œåˆ™æ¸…ç©ºæ—¥å¿—æ–‡ä»¶ã€‚
    å¦‚æœå¯ç”¨äº† --log å‚æ•°ä½†æœªæŒ‡å®šæ–‡ä»¶åï¼Œåˆ™ä½¿ç”¨é»˜è®¤æ–‡ä»¶åã€‚

    å‚æ•°:
        config (dict): åŒ…å«æ—¥å¿—è®¾ç½®çš„é…ç½®å­—å…¸
    """
    if config.get("log") and not config.get("log_file"):
        config["log_file"] = "obsidian_import.log"

    if config.get("reset_log", False) and config.get("log_file"):
        mode = "w" if config.get("reset_log") else "a"
        log_file_handle = safe_open_file(config["log_file"], mode)
        if log_file_handle:
            config["log_file_handle"] = log_file_handle
            print(f"âœ… Log file {config['log_file']} {'reset' if mode == 'w' else 'opened for appending'} successfully.")
            debug("Log file reset by user request", LOG_LEVEL_ACTION, config)

def close_log_file(config: Dict[str, Any]) -> None:
    """
    å…³é—­æ—¥å¿—æ–‡ä»¶å¥æŸ„ã€‚

    å‚æ•°:
        config (dict): é…ç½®å­—å…¸
    """
    log_file_handle = config.get("log_file_handle")
    safe_close_file(log_file_handle)

#############################################################
# PROGRESS BAR FUNCTION
#############################################################
def display_progress_bar(current: int, total: int, description: str = "") -> None:
    """
    æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œæ ¼å¼ä¸º: XXXX: XXXX [####----] 33% ETA 01:23 å½“å‰ä»»åŠ¡æç¤º

    å‚æ•°:
        current (int): å½“å‰è¿›åº¦
        total (int): æ€»ä»»åŠ¡æ•°
        description (str): å½“å‰ä»»åŠ¡çš„æè¿°
        width (int, optional): ç»ˆç«¯æ€»å®½åº¦ï¼Œé»˜è®¤ä¸ºç»ˆç«¯å®½åº¦
    """
    try:
        terminal_width = min(100, shutil.get_terminal_size().columns)
    except:
        terminal_width = 80  # é»˜è®¤å®½åº¦

    # å›ºå®šå¸ƒå±€å®½åº¦
    left_width = 10  # å·¦ä¾§ "XXXX: XXXX" çš„å®½åº¦
    progress_bar_width = 20  # è¿›åº¦æ¡å®½åº¦
    percent_eta_width = 15  # ç™¾åˆ†æ¯”å’Œ ETA çš„å®½åº¦ ("33% ETA 01:23")
    description_width = min(20, terminal_width - left_width - progress_bar_width - percent_eta_width - 10)  # é¢„ç•™ç©ºæ ¼

    # é™åˆ¶æè¿°é•¿åº¦å¹¶æ·»åŠ çœç•¥å·
    if len(description) > description_width:
        description = description[:description_width - 3] + "..."

    # è®¡ç®—å®Œæˆç™¾åˆ†æ¯”
    percent = current / total
    percent_str = f"{percent * 100:3.0f}%"  # ç™¾åˆ†æ¯”å­—ç¬¦ä¸²

    # è®¡ç®— ETA (é¢„è®¡å‰©ä½™æ—¶é—´)
    if not hasattr(display_progress_bar, "start_time"):
        display_progress_bar.start_time = time.time()

    elapsed = time.time() - display_progress_bar.start_time
    if current > 0:
        eta_seconds = (elapsed / current) * (total - current)
        eta_min, eta_sec = divmod(int(eta_seconds), 60)
        eta_str = f"ETA {eta_min:02d}:{eta_sec:02d}"
    else:
        eta_str = "ETA --:--"

    # æ„å»ºè¿›åº¦æ¡å­—ç¬¦ä¸²
    completed = int(progress_bar_width * percent)
    progress_bar = "#" * completed + "-" * (progress_bar_width - completed)

    # æ„å»ºå®Œæ•´çš„è¿›åº¦æ˜¾ç¤º
    progress_str = f"{current:4}/{total:<4} [{progress_bar}] {percent_str} {eta_str} {description}"
    if len(progress_str) > terminal_width:
        progress_str = progress_str[:terminal_width - 3] + "..."

    # æ¸…é™¤å½“å‰è¡Œå¹¶æ˜¾ç¤ºè¿›åº¦
    sys.stdout.write("\r" + " " * terminal_width)  # æ¸…é™¤æ•´è¡Œ
    sys.stdout.write("\r" + progress_str)
    sys.stdout.flush()

    # ä¿å­˜æœ€åæ˜¾ç¤ºçš„è¡Œï¼Œä»¥ä¾¿ä¸‹æ¬¡æ¸…é™¤
    display_progress_bar.last_line = progress_str

    # å¦‚æœå®Œæˆï¼Œæ·»åŠ æ¢è¡Œ
    if current == total:
        sys.stdout.write("\n")
        sys.stdout.flush()
        if hasattr(display_progress_bar, "start_time"):
            delattr(display_progress_bar, "start_time")


#############################################################
# CONFIGURATION AND LOGGING FUNCTIONS
#############################################################

# Functions in this section handle configuration loading and logging.
# These include `load_config`, `trace_debug`, and `log_debug`.

def load_config(config_path: str) -> Dict[str, Any]:
    """
    ä» YAML æ–‡ä»¶åŠ è½½é…ç½®ã€‚

    å‚æ•°:
        config_path (str): é…ç½®æ–‡ä»¶çš„è·¯å¾„

    è¿”å›:
        dict: é…ç½®å­—å…¸ï¼Œå¦‚æœåŠ è½½å¤±è´¥åˆ™è¿”å›ç©ºå­—å…¸
    """
    file_handle = safe_open_file(config_path, "r")
    if file_handle:
        with file_handle:
            return yaml.safe_load(file_handle)
    return {}

#############################################################
# METADATA VALIDATION AND TASK GENERATION
#############################################################

# Functions in this section handle metadata validation and task generation.
# These include `validate_metadata_mappings`, `read_metadata_lines`,
# `process_metadata_line`, `apply_metadata_actions`, and `generate_metadata_tasks`.

def validate_metadata_mappings(lines: List[str], config: Dict[str, Any]) -> None:
    """
    æ ¹æ®è§„åˆ™éªŒè¯å…ƒæ•°æ®å€¼å¹¶è·Ÿè¸ªæœªæ˜ å°„çš„å…ƒæ•°æ®ã€‚

    å‚æ•°:
        lines (list): Markdown æ–‡ä»¶ä¸­çš„è¡Œåˆ—è¡¨
        config (dict): é…ç½®å­—å…¸ï¼ŒåŒ…å«å…ƒæ•°æ®è§„åˆ™å’Œæœªæ˜ å°„å…ƒæ•°æ®çš„å­˜å‚¨
    """
    metadata_rules = config.get("metadata_rules", {})
    unmapped_metadata = config.setdefault("unmapped_metadata", {})
    
    for line in lines:
        key, sep, value = line.partition(": ")
        if not sep:  # Skip lines that are not metadata
            continue

        rule = metadata_rules.get(key)
        if not rule:  # No rule, skip validation
            continue

        actions = rule.get("actions", [])
        for action in actions:
            if action.get("type") == "modify_value":
                value_mapping = action.get("value_mapping", {})
                if value.strip() not in value_mapping and value.strip() not in unmapped_metadata.get(key, set()):
                    unmapped_metadata.setdefault(key, set()).add(value.strip())
    debug(f"éªŒè¯å…ƒæ•°æ®æ˜ å°„å®Œæˆ: {unmapped_metadata}", LOG_LEVEL_DEBUG, config)

def read_metadata_lines(md_file: Union[str, Path], config: Dict[str, Any]) -> List[str]:
    """
    ä» Markdown æ–‡ä»¶ä¸­æå–å…ƒæ•°æ®è¡Œã€‚

    å‚æ•°:
        md_file (str æˆ– Path): Markdown æ–‡ä»¶çš„è·¯å¾„
        config (dict): é…ç½®å­—å…¸ï¼ŒåŒ…å«å…ƒæ•°æ®è§„åˆ™
    """
    metadata_rules = config.get("metadata_rules", {})
    with open(md_file, "r", encoding="utf-8") as f:
        content = f.readlines()

    debug(f"Reading metadata lines from {md_file}", LOG_LEVEL_DEBUG, config)

    # Step 1: Read the first 10 lines
    first_10_lines = content[:10]

    # Step 2: Check for a match in metadata_rules
    metadata_start = -1
    for i, line in enumerate(first_10_lines):
        if line.strip().startswith("#"):  # Skip comment lines
            continue
        key, sep, _ = line.partition(": ")
        if sep and key.strip() in metadata_rules:
            metadata_start = i
            break

    if metadata_start == -1:
        debug(f"No metadata match found in the first 10 lines of {md_file}", LOG_LEVEL_DEBUG, config)
        return []

    # Step 3: Expand the metadata region
    metadata_lines = []
    # Search backward
    for i in range(metadata_start, -1, -1):
        line = content[i].strip()
        if not line or line == "---":  # Stop at empty or "---" lines
            break
        if ": " in line:  # Include lines with "key: value" format
            metadata_lines.insert(0, line)

    # Search forward
    for i in range(metadata_start + 1, len(content)):
        line = content[i].strip()
        if not line or line == "---":  # Stop at empty or "---" lines
            break
        if ": " in line:  # Include lines with "key: value" format
            metadata_lines.append(line)

    debug(f"ä» {md_file} æå–çš„å…ƒæ•°æ®è¡Œ: {metadata_lines}", LOG_LEVEL_DEBUG, config)
    return metadata_lines

def process_metadata_line(line: str, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    å¤„ç†å•è¡Œå…ƒæ•°æ®å¹¶æ ¹æ®è§„åˆ™ç”Ÿæˆä»»åŠ¡ã€‚

    å‚æ•°:
        line (str): å•è¡Œå…ƒæ•°æ®
        config (dict): é…ç½®å­—å…¸ï¼ŒåŒ…å«å…ƒæ•°æ®è§„åˆ™
    """
    metadata_rules = config.get("metadata_rules", {})
    key, sep, value = line.partition(": ")
    if not sep:
        return []

    debug(f"å¼€å§‹å¤„ç†å…ƒæ•°æ®è¡Œâ€¦â€¦: {line}", LOG_LEVEL_DEBUG, config)

    value = value.strip()
    matching_keys = [rule_key for rule_key in metadata_rules if key.startswith(rule_key)]
    if not matching_keys:
        debug(f"âš ï¸ Warning: No rule defined for metadata key '{key}'.", LOG_LEVEL_ERROR, config)
        return []

    rule = metadata_rules[matching_keys[0]]
    actions = rule.get("actions", [])

    if any(action.get("type") == "delete" for action in actions):
        task = {
            "type": TaskType.TRANSFORM_METADATA.value,
            "key": key,
            "action": {"type": "delete"},
            "file": config.get("current_file"),  # æ·»åŠ æ–‡ä»¶è·¯å¾„
            "id": generate_task_id()  # Add unique task ID
        }
        debug(f"â• Added metadata task: {task}", LOG_LEVEL_ACTION, config)
        return [task]

    if any(action.get("type") == "retain" for action in actions) and len(actions) == 1:
        return []

    tasks = []

    # Handle `insert` actions
    for action in actions:
        if action.get("type") == "insert":
            tasks.append({
                "type": "INSERT_CONTENT",
                "position": action.get("at", "before"),  # Use 'at' parameter for position
                "content": action.get("content", ""),
                "key": key,
                "file": config.get("current_file"),
                "id": generate_task_id(),
                "is_pre_task": True,  # Mark as a preprocessing task
                "status": "todo"
            })

    # Handle other metadata actions
    tasks.extend(apply_metadata_actions(key, value, actions, config))
    
    # Log each metadata task that was generated
    for task in tasks:
        debug(f"â• Added metadata task: {task}", LOG_LEVEL_ACTION, config)
        
    return tasks

def apply_metadata_actions(key: str, value: str, actions: List[Dict[str, Any]], config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    å¯¹é”®å€¼å¯¹åº”ç”¨å…ƒæ•°æ®æ“ä½œï¼ˆå¦‚ modify_valueã€append_afterã€renameï¼‰ã€‚

    å‚æ•°:
        key (str): å…ƒæ•°æ®é”®
        value (str): å…ƒæ•°æ®å€¼
        actions (list): è¦åº”ç”¨çš„æ“ä½œåˆ—è¡¨

    è¿”å›:
        list: è½¬æ¢ä»»åŠ¡åˆ—è¡¨
    """
    tasks = []
    current_key, current_value = key, value
    modified = False

    sorted_actions = sorted(
        [a for a in actions if a.get("type") not in ["retain", "delete"]],
        key=lambda x: {"modify_value": 0, "append_after": 1, "rename": 2}.get(x.get("type"), 999)
    )

    for action in sorted_actions:
        action_type = action.get("type")
        if action_type == "modify_value":
            value_mapping = action.get("value_mapping", {})
            regex_mapping = action.get("regex_mapping", [])
            if current_value in value_mapping:
                current_value = value_mapping[current_value]
                modified = True
            else:
                for regex, replacement in regex_mapping:
                    match = re.search(regex, current_value)
                    if match:
                        # Check if the regex contains capturing groups
                        if match.groups():
                            # Use the regex replacement with capture groups
                            current_value = re.sub(regex, replacement, current_value)
                        else:
                            # No capture groups, use replacement as default
                            current_value = replacement
                        modified = True
                        break
        elif action_type == "append_after":
            current_value += action.get("content", "")
            modified = True
        elif action_type == "rename":
            current_key = action.get("new_name", key)
            modified = True

    if modified:
        tasks.append({
            "type": TaskType.TRANSFORM_METADATA.value,
            "key": key,
            "old": f"{key}: {value}",
            "new": f"{current_key}: {current_value}",
            "action": {"type": "replace"},
            "file": config.get("current_file"),  # Ensure 'file' key is included
            "id": generate_task_id()  # Add unique task ID
        })
    return tasks

def generate_metadata_tasks(md_file: Union[str, Path], config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    è§£æ Markdown æ–‡ä»¶ä¸­çš„å…ƒæ•°æ®å¹¶ç”Ÿæˆè½¬æ¢ä»»åŠ¡ã€‚

    å‚æ•°:
        md_file (str æˆ– Path): Markdown æ–‡ä»¶çš„è·¯å¾„
        config (dict): é…ç½®å­—å…¸ï¼ŒåŒ…å«å…ƒæ•°æ®è§„åˆ™

    è¿”å›:
        list: è½¬æ¢ä»»åŠ¡åˆ—è¡¨
    """
    tasks = []
    metadata_lines = read_metadata_lines(md_file, config)
    for line in metadata_lines:
        tasks.extend(process_metadata_line(line, config))
    for task in tasks:
        task["status"] = "todo"  # Initialize task status as "todo"
    return tasks

#############################################################
# DIRECTORY SCANNING AND TASK PLANNING
#############################################################

# Functions in this section handle scanning directories and planning tasks.
# These include `initialize_scan_stats`, `scan_markdown_file`,
# `scan_attachments`, `generate_rename_markdown_task`, and `scan_directory`.

def initialize_scan_stats() -> Dict[str, Any]:
    """
    åˆå§‹åŒ–æ‰«æè¿‡ç¨‹çš„ç»Ÿè®¡ä¿¡æ¯å’Œå˜é‡ã€‚

    è¿”å›:
        tuple: åŒ…å«ç»Ÿè®¡ä¿¡æ¯å­—å…¸ã€æœªæ˜ å°„å…ƒæ•°æ®å­—å…¸å’Œå·²ä½¿ç”¨åç§°é›†åˆçš„å…ƒç»„
    """
    return {
        "markdown_files": 0,
        "attachments": 0,
        "conflicts": 0,
        "metadata_tasks": 0,
        "unmapped_metadata": {},  # ç”¨äºå­˜å‚¨æœªæ˜ å°„çš„å…ƒæ•°æ®
        "used_names": set(),      # ç”¨äºå­˜å‚¨å·²ä½¿ç”¨çš„æ–‡ä»¶å
        "errors": 0               # ç”¨äºç»Ÿè®¡é”™è¯¯æ—¥å¿—æ•°é‡
    }


def scan_markdown_file(file: str, root: str, directory: str, resource_dir: Path, tasks: List[Dict[str, Any]], config: Dict[str, Any]) -> None:
    """
    å¤„ç†å•ä¸ª Markdown æ–‡ä»¶ã€‚

    å‚æ•°:
        file (str): Markdown æ–‡ä»¶çš„åç§°
        root (str): æ–‡ä»¶çš„æ ¹ç›®å½•
        directory (str): æ­£åœ¨æ‰«æçš„åŸºç›®å½•
        resource_dir (Path): èµ„æºç›®å½•çš„è·¯å¾„
        tasks (list): ç”¨äºæ”¶é›†ç”Ÿæˆä»»åŠ¡çš„åˆ—è¡¨
        config (dict): é…ç½®å­—å…¸
    """
    debug("------------------------------------------------------------", LOG_LEVEL_DEBUG, config)
    debug(f"ğŸ” Processing Markdown file: {file}", LOG_LEVEL_DEBUG, config)

    original_path = Path(root) / file
    config["stats"]["markdown_files"] += 1

    # Step 1: Add metadata mapping tasks
    debug("ğŸ› ï¸ 1.Generating metadata transformation tasks...", LOG_LEVEL_DEBUG, config)  # Changed to DEBUG
    config["current_file"] = str(original_path)  # è®¾ç½®å½“å‰æ–‡ä»¶è·¯å¾„
    metadata_tasks = generate_metadata_tasks(original_path, config)

    if metadata_tasks:
        # è·å–ç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡å…ƒæ•°æ®ä»»åŠ¡çš„ key
        first_metadata_key = metadata_tasks[0].get("key")
        last_metadata_key = metadata_tasks[-1].get("key")

        # æ ¹æ® config æ’å…¥å…ƒæ•°æ®åˆ†éš”ç¬¦ä»»åŠ¡
        metadata_section_rules = config.get("metadata_section_rules", [])
        for rule in metadata_section_rules:
            if rule["type"] == "insert" and rule["position"] == "first":
                tasks.append({
                    "type": "INSERT_CONTENT",
                    "file": original_path,
                    "position": "before",
                    "content": rule["content"],
                    "key": first_metadata_key,
                    "is_pre_task": True,
                    "status": "todo",
                    "id": generate_task_id()
                })
                debug(f"â• Added metadata section start task for {original_path}", LOG_LEVEL_ACTION, config)

            if rule["type"] == "insert" and rule["position"] == "last":
                tasks.append({
                    "type": "INSERT_CONTENT",
                    "file": original_path,
                    "position": "after",
                    "content": rule["content"],
                    "key": last_metadata_key,
                    "is_pre_task": True,
                    "status": "todo",
                    "id": generate_task_id()
                })
                debug(f"â• Added metadata section end task for {original_path}", LOG_LEVEL_ACTION, config)

        # æ·»åŠ å…ƒæ•°æ®ä»»åŠ¡
        tasks.extend(metadata_tasks)
        config["stats"]["metadata_tasks"] += len(metadata_tasks)

    # Step 2: Process attachments
    debug("ğŸ“¦ 2.Processing attachments...", LOG_LEVEL_DEBUG, config)  # Changed to DEBUG
    file_path_mapping = scan_attachments(original_path, directory, resource_dir, tasks, config)

    # Step 3: Update references in Markdown file
    debug("ğŸ”— 3.Updating references in Markdown file...", LOG_LEVEL_DEBUG, config)
    if file_path_mapping:  # Only add the task if path_mapping is not empty
        update_task = {
            "type": TaskType.UPDATE_ATTACH_REF.value,
            "file": original_path,
            "path_mapping": file_path_mapping,  # Use file-specific path_mapping
            "id": generate_task_id()  # Add unique task ID
        }
        debug(f"â• Added update references task: {update_task}", LOG_LEVEL_ACTION, config)
        tasks.append(update_task)

    # Step 4: Rename Markdown file
    debug("âœï¸ 4.Renaming Markdown file...", LOG_LEVEL_DEBUG, config)
    rename_task = generate_rename_markdown_task(original_path, directory, tasks)
    if rename_task:
        debug(f"â• Added rename task: {rename_task}", LOG_LEVEL_ACTION, config)

    debug(f"âœ… 5.Finished processing Markdown file: {file}", LOG_LEVEL_DEBUG, config)

def scan_attachments(original_path: Path, directory: str, resource_dir: Path, tasks: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, str]:
    """
    å¤„ç†ç»™å®š Markdown æ–‡ä»¶çš„é™„ä»¶ï¼ŒåŒ…æ‹¬å›¾ç‰‡ã€è§†é¢‘å’Œ PDFã€‚

    å‚æ•°:
        original_path (Path): Markdown æ–‡ä»¶çš„è·¯å¾„
        directory (str): æ­£åœ¨æ‰«æçš„åŸºç›®å½•
        resource_dir (Path): èµ„æºç›®å½•çš„è·¯å¾„
        tasks (list): ç”¨äºæ”¶é›†ç”Ÿæˆä»»åŠ¡çš„åˆ—è¡¨
        config (dict): é…ç½®å­—å…¸

    è¿”å›:
        dict: æ—§é™„ä»¶è·¯å¾„åˆ°æ–°è·¯å¾„çš„æ˜ å°„
    """
    # Extract UID from the Markdown filename
    uid_match = re.search(r" (\w{32})\.md$", str(original_path))
    attachment_dir = None

    if uid_match:
        uid = uid_match.group(1)
        debug(f"ğŸ“Œ Found UID in Markdown filename: {uid}", LOG_LEVEL_DEBUG, config)

        # Look for an attachment directory with matching UID in its name
        parent_dir = original_path.parent
        potential_dirs = [d for d in parent_dir.iterdir() if d.is_dir()]

        for pot_dir in potential_dirs:
            if uid in pot_dir.name:
                attachment_dir = pot_dir
                debug(f"ğŸ“‚ Found matching attachment directory: {attachment_dir}", LOG_LEVEL_DEBUG, config)
                break

    # Step 1: Check if the Markdown file contains attachment references
    content = ""
    with open(original_path, "r") as f:
        content = f.read()
    if not content:
        debug(f"Error: Empty Markdown file {original_path}, skipping attachment processing", LOG_LEVEL_ERROR, config)
        return {}
    
    # Match Markdown links for images, videos, and PDFs
    attachment_references = re.findall(r"!?\[.*?\]\((.*?)\)", content)  # Match Markdown image/video/PDF links

    # Filter out network attachments (e.g., http:// or https://)
    local_references = []
    for ref in attachment_references:
        if ref.startswith("http://") or ref.startswith("https://"):
            debug(f"ğŸŒ Ignoring network attachment: {ref}", LOG_LEVEL_DEBUG, config)
        else:
            local_references.append(ref)

    if not local_references:
        debug(f"â„¹ï¸ No local attachment references found in {original_path.name}, skipping attachment processing", LOG_LEVEL_DEBUG, config)
        return {}  # No local references, no need to process attachments

    # Step 2: Handle references to attachments
    path_mapping = {}
    moved_files_count = 0  # Track the number of moved files

    # Supported attachment extensions
    supported_extensions = {".png", ".jpg", ".jpeg", ".gif", ".bmp", ".mp4", ".mov", ".avi", ".mkv", ".pdf", ".heic", ".webp", ".qt", ".m4a", ".mp3", ".wav"}

    for ref in local_references:
        ref_path = Path(directory) / unquote(ref)
        ref_path_type = probe_path(ref_path, config)
        if ref_path_type in ["not_exist", "error"]:
            debug(f"Attachment not found: {ref}", LOG_LEVEL_ERROR, config)
            config["stats"]["errors"] += 1
            continue

        if ref_path.is_file():
            if ref_path.suffix.lower() not in supported_extensions:
                debug(f"Unsupported attachment extension: {ref_path.suffix} in file {ref_path}", LOG_LEVEL_ERROR, config)
                config["stats"]["errors"] += 1  # Increment error count
                continue     

            if attachment_dir and ref_path.parent == attachment_dir:
                # å½“å‰ Markdown æ–‡ä»¶çš„é™„ä»¶
                moved_files_count += 1
                new_attachment_name = f"{original_path.stem}_{ref_path.name}"
                new_attachment_path = resource_dir / new_attachment_name
                old_path = quote(str(ref_path.relative_to(directory)).replace("\\", "/"))
                new_path = str(new_attachment_path.relative_to(directory)).replace("\\", "/")
                path_mapping[old_path] = new_path

                move_task = {
                    "type": TaskType.MOVE_ATTACHMENT.value,
                    "src": ref_path,
                    "dest": new_attachment_path,
                    "is_pre_task": False,
                    "status": "todo",  # Initialize task status as "todo"
                    "id": generate_task_id()  # Add unique task ID
                }
                debug(f"â• Added move attachment task: {move_task}", LOG_LEVEL_ACTION, config)
                tasks.append(move_task)
            else:
                # å…¶ä»– Markdown æ–‡ä»¶çš„é™„ä»¶
                base_name = original_path.stem
                new_attachment_name = base_name + ref_path.suffix
                counter = 1
                while (resource_dir / new_attachment_name).exists():
                    new_attachment_name = f"{base_name}_{counter}{ref_path.suffix}"
                    counter += 1

                new_attachment_path = resource_dir / new_attachment_name
                old_path = quote(str(ref_path.relative_to(directory)).replace("\\", "/"))
                new_path = str(new_attachment_path.relative_to(directory)).replace("\\", "/")
                
                # æ›´æ–° path_mappingï¼Œç¡®ä¿æ¯ä¸ªæ–‡ä»¶çš„å¼•ç”¨ç‹¬ç«‹
                if old_path not in path_mapping or path_mapping[old_path] != new_path:
                    path_mapping[old_path] = new_path

                copy_task = {
                    "type": TaskType.COPY_ATTACHMENT.value,
                    "src": ref_path,
                    "dest": new_attachment_path,
                    "is_pre_task": True,
                    "status": "todo",  # Initialize task status as "todo"
                    "id": generate_task_id()  # Add unique task ID
                }
                debug(f"â• Added copy attachment task: {copy_task}", LOG_LEVEL_ACTION, config)
                tasks.append(copy_task)

    # Step 3: Add cleanup task if all files are moved
    if attachment_dir and attachment_dir.exists():
        attachment_files = list(attachment_dir.iterdir())
        if moved_files_count == len(attachment_files):
            cleanup_task = {
                "type": TaskType.CLEANUP.value,
                "dest": attachment_dir,
                "id": generate_task_id()  # Add unique task ID
            }
            debug(f"â• Added cleanup task: {cleanup_task}", LOG_LEVEL_ACTION, config)
            tasks.append(cleanup_task)

    return path_mapping

def generate_rename_markdown_task(original_path: Path, directory: str, tasks: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
    """
    é€šè¿‡ç§»é™¤ UID å¹¶è§£å†³å†²çªæ¥é‡å‘½å Markdown æ–‡ä»¶ã€‚

    å‚æ•°:
        original_path (Path): Markdown æ–‡ä»¶çš„è·¯å¾„
        directory (str): æ­£åœ¨æ‰«æçš„åŸºç›®å½•
        tasks (list): ç”¨äºæ”¶é›†ç”Ÿæˆä»»åŠ¡çš„åˆ—è¡¨

    è¿”å›:
        dict: æ·»åŠ åˆ°ä»»åŠ¡åˆ—è¡¨ä¸­çš„é‡å‘½åä»»åŠ¡
    """
    base_name = re.sub(r" \w{32}$", "", original_path.stem)  # Remove UID
    new_name = base_name
    counter = 1
    while new_name in [task.get("dest", "").stem for task in tasks if task["type"] == TaskType.RENAME_MD.value]:
        new_name = f"{base_name}_{counter}"
        counter += 1
    new_md_path = Path(directory) / f"{new_name}.md"
    rename_task = {
        "type": TaskType.RENAME_MD.value,
        "src": original_path,
        "dest": new_md_path,
        "status": "todo",  # Initialize task status as "todo"
        "id": generate_task_id()  # Add unique task ID
    }
    tasks.append(rename_task)
    return rename_task


def scan_directory(directory: str, attachment_output_path: str, config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    æ‰«æç›®å½•ä¸­çš„ Markdown æ–‡ä»¶å¹¶ç”Ÿæˆå¤„ç†ä»»åŠ¡ã€‚

    å‚æ•°:
        directory (str): è¦æ‰«æçš„ç›®å½•è·¯å¾„
        attachment_output_path (str): èµ„æºç›®å½•çš„åç§°
        metadata_rules (dict): å¤„ç†å…ƒæ•°æ®çš„è§„åˆ™
        config (dict): é…ç½®å­—å…¸

    è¿”å›:
        list: ç”Ÿæˆçš„ä»»åŠ¡åˆ—è¡¨
    """
    tasks = []
    config["stats"] = initialize_scan_stats()  # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
    resource_dir = Path(directory) / attachment_output_path
    resource_dir.mkdir(exist_ok=True)

    debug(f"ğŸ“‚ Resource directory created at: {resource_dir}", LOG_LEVEL_DEBUG, config)

    # è·å–æ‰€æœ‰ Markdown æ–‡ä»¶æ•°é‡ç”¨äºè¿›åº¦æ¡
    if not config.get("debug", False):
        md_files = []
        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith(".md"):
                    md_files.append(os.path.join(root, file))
        total_files = len(md_files)
        current_file = 0

        # é‡ç½®è¿›åº¦æ¡è®¡æ—¶å™¨
        if hasattr(display_progress_bar, "start_time"):
            delattr(display_progress_bar, "start_time")

    # æ‰«æ Markdown æ–‡ä»¶
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".md"):
                if not config.get("debug", False):
                    current_file += 1
                    display_progress_bar(current_file, total_files, f"æ‰«æ: {file}")

                debug(f"ğŸ“„ Found Markdown file: {file}", LOG_LEVEL_DEBUG, config)
                scan_markdown_file(file, root, directory, resource_dir, tasks, config)

    return tasks

#############################################################
# TASK EXECUTION
#############################################################

# Functions in this section handle task execution.
# These include `execute_task` and `execute_tasks`.

def generate_task_id() -> str:
    """
    Generate a unique task ID in the format 'taskxxxxx'.
    """
    return f"task{uuid.uuid4().hex[:5]}"

def execute_task(task: Dict[str, Any], config: Dict[str, Any]) -> None:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œå•ä¸ªä»»åŠ¡ã€‚

    å‚æ•°:
        task (dict): è¦æ‰§è¡Œçš„ä»»åŠ¡
        config (dict): é…ç½®å­—å…¸
    """
    if task.get("status") in ["done", "fail"]:
        return
    try:
        if "id" not in task:
            debug(f"âš ï¸ Task ID not found, generating a new one.", LOG_LEVEL_ERROR, config)
            task["id"] = generate_task_id()  # Ensure task has a unique ID
        if task["type"] == TaskType.RENAME_MD.value:
            debug(f"âœï¸ é‡å‘½åæ–‡ä»¶: {task}", LOG_LEVEL_ACTION, config)
            Path(task["src"]).rename(task["dest"])
        elif task["type"] == TaskType.MOVE_ATTACHMENT.value:
            debug(f"ğŸ“¦ ç§»åŠ¨é™„ä»¶: {task}", LOG_LEVEL_ACTION, config)
            Path(task["src"]).rename(task["dest"])
        elif task["type"] == TaskType.COPY_ATTACHMENT.value:
            debug(f"ğŸ“‹ å¤åˆ¶é™„ä»¶: {task}", LOG_LEVEL_ACTION, config)
            shutil.copy(task["src"], task["dest"])  # æ‰§è¡Œå¤åˆ¶æ“ä½œ
        elif task["type"] == TaskType.UPDATE_ATTACH_REF.value:
            debug(f"ğŸ”— æ›´æ–°æ–‡ä»¶ä¸­çš„å¼•ç”¨: {task}", LOG_LEVEL_ACTION, config)
            path_mapping = task.get("path_mapping", {})  # ä»ä»»åŠ¡ä¸­è·å– path_mapping
            update_references_in_markdown(task["file"], path_mapping, config)
        elif task["type"] == TaskType.TRANSFORM_METADATA.value:
            debug(f"ğŸ› ï¸ è½¬æ¢æ–‡ä»¶ä¸­çš„å…ƒæ•°æ®: {task}", LOG_LEVEL_ACTION, config)
            map_metadata(task["file"], config)
        elif task["type"] == "INSERT_CONTENT":
            debug(f"âœï¸ æ’å…¥å†…å®¹: {task}", LOG_LEVEL_ACTION, config)
            insert_content_in_file(task, config)
        elif task["type"] == TaskType.CLEANUP.value:
            debug(f"ğŸ—‘ï¸ æ¸…ç†ç›®æ ‡: {task}", LOG_LEVEL_ACTION, config)
            dest_path = Path(task["dest"])  # ç¡®ä¿ dest æ˜¯ Path å¯¹è±¡
            if dest_path.exists():
                if dest_path.is_file():
                    dest_path.unlink()  # åˆ é™¤æ–‡ä»¶
                    debug(f"âœ… æ–‡ä»¶å·²åˆ é™¤: {dest_path}", LOG_LEVEL_ACTION, config)
                elif dest_path.is_dir():
                    try:
                        dest_path.rmdir()  # å°è¯•åˆ é™¤ç©ºç›®å½•
                        debug(f"âœ… ç©ºç›®å½•å·²åˆ é™¤: {dest_path}", LOG_LEVEL_ACTION, config)
                    except OSError:
                        shutil.rmtree(dest_path)  # åˆ é™¤éç©ºç›®å½•
                        debug(f"âœ… éç©ºç›®å½•å·²åˆ é™¤: {dest_path}", LOG_LEVEL_ACTION, config)
            else:
                debug(f"âš ï¸ æ¸…ç†ç›®æ ‡ä¸å­˜åœ¨: {dest_path}", LOG_LEVEL_ERROR, config)
        task["status"] = "done"  # Mark task as done
        debug(f"âœ… Task completed: {task['type']} (ID: {task['id']}, Status: {task['status']})", LOG_LEVEL_DEBUG, config)
    except Exception as e:
        debug(f"Task failed: {task}. Error: {e}", LOG_LEVEL_ERROR, config)
        task["status"] = "fail"  # Mark task as failed
        raise e

def execute_tasks(tasks: List[Dict[str, Any]], config: Dict[str, Any]) -> None:
    """
    æŒ‰ç”Ÿæˆé¡ºåºæ‰§è¡Œä»»åŠ¡åˆ—è¡¨ã€‚

    å‚æ•°:
        tasks (list): è¦æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨
        config (dict): é…ç½®å­—å…¸
    """
    # é‡ç½®è¿›åº¦æ¡è®¡æ—¶å™¨
    if hasattr(display_progress_bar, "start_time"):
        delattr(display_progress_bar, "start_time")
    
    # ä¼˜å…ˆæ‰§è¡Œé¢„å¤„ç†ä»»åŠ¡
    pre_tasks = [task for task in tasks if task.get("is_pre_task", False)]
    main_tasks = [task for task in tasks if not task.get("is_pre_task", False)]

    debug(f"âš™ï¸ Executing {len(pre_tasks)} pre-tasks...", LOG_LEVEL_FLOW, config)
    for i, task in enumerate(pre_tasks, start=1):
        if "id" not in task:
            task["id"] = generate_task_id()  # Ensure task has a unique ID
        debug(f"âš™ï¸ Executing pre-task {i}/{len(pre_tasks)}: {task['type']}", LOG_LEVEL_DEBUG, config)
        execute_task(task, config)

    debug(f"âš™ï¸ Executing {len(main_tasks)} main tasks...", LOG_LEVEL_FLOW, config)
    for i, task in enumerate(main_tasks, start=1):
        if "id" not in task:
            task["id"] = generate_task_id()  # Ensure task has a unique ID
        task_type = task['type']
        task_desc = ""

        if task_type == TaskType.RENAME_MD.value:
            task_desc = f"é‡å‘½å: {Path(task['src']).name}"
        elif task_type == TaskType.MOVE_ATTACHMENT.value:
            task_desc = f"ç§»åŠ¨é™„ä»¶: {Path(task['src']).name}"
        elif task_type == TaskType.UPDATE_ATTACH_REF.value:
            task_desc = f"æ›´æ–°å¼•ç”¨: {Path(task['file']).name}"
        elif task_type == TaskType.TRANSFORM_METADATA.value:
            task_desc = f"è½¬æ¢å…ƒæ•°æ®"
        
        if not config.get("debug", False):
            display_progress_bar(i, len(main_tasks), task_desc)
        
        debug(f"âš™ï¸ Executing task {i}/{len(main_tasks)}: {task['type']}", LOG_LEVEL_DEBUG, config)
        execute_task(task, config)

#############################################################
# METADATA TRANSFORMATION
#############################################################

# Functions in this section handle metadata transformation.
# These include `apply_action`, `transform_metadata`, `update_references_in_markdown`, and `map_metadata`.

def insert_content_in_file(task: Dict[str, Any], config: Dict[str, Any]) -> None:
    """
    åœ¨æ–‡ä»¶ä¸­æ’å…¥å†…å®¹ã€‚

    å‚æ•°:
        task (dict): åŒ…å«æ’å…¥å†…å®¹çš„ä»»åŠ¡
        config (dict): é…ç½®å­—å…¸
    """
    file_path = task["file"]
    position = task["position"]
    content = task["content"]
    key = task["key"]

    file_handle = safe_open_file(file_path, "r")
    if not file_handle:
        return

    with file_handle:
        lines = file_handle.readlines()

    updated_lines = []
    for line in lines:
        if position == "before" and line.startswith(f"{key}:"):
            updated_lines.append(content + "\n")
        updated_lines.append(line)
        if position == "after" and line.startswith(f"{key}:"):
            updated_lines.append(content + "\n")

    file_handle = safe_open_file(file_path, "w")
    if file_handle:
        with file_handle:
            file_handle.writelines(updated_lines)

    task["status"] = "done"  # Mark task as done
    debug(f"âœ… æ’å…¥å†…å®¹å®Œæˆ: {task}", LOG_LEVEL_ACTION, config)

def apply_action(key: str, value: str, action: Dict[str, Any], config: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:
    """
    å¯¹é”®å€¼å¯¹åº”ç”¨å…ƒæ•°æ®è½¬æ¢æ“ä½œã€‚

    å‚æ•°:
        key (str): å…ƒæ•°æ®é”®
        value (str): å…ƒæ•°æ®å€¼
        action (dict): è¦åº”ç”¨çš„æ“ä½œ
        config (dict): é…ç½®å­—å…¸

    è¿”å›:
        tuple: (æ–°é”®, æ–°å€¼) æˆ– (None, None)ï¼ˆå¦‚æœå…ƒæ•°æ®åº”è¢«åˆ é™¤ï¼‰
    """
    debug(f"ğŸ”§ Applying action '{action.get('type')}' on key '{key}' with value '{value}'", LOG_LEVEL_DEBUG, config)
    action_type = action.get("type")
    if action_type == "delete":
        debug(f"ğŸ—‘ï¸ Deleting metadata key: {key}", LOG_LEVEL_ACTION, config)
        return None, None  # Indicate deletion
    elif action_type == "rename":
        new_name = action.get("new_name", key)
        debug(f"âœï¸ Renaming key '{key}' to '{new_name}'", LOG_LEVEL_ACTION, config)
        return new_name, value
    elif action_type == "modify_value":
        value_mapping = action.get("value_mapping", {})
        regex_mapping = action.get("regex_mapping", [])
        new_value = value.strip()

        # Apply direct value mapping
        if new_value in value_mapping:
            debug(f"ğŸ”„ Mapping value '{new_value}' to '{value_mapping[new_value]}'", LOG_LEVEL_ACTION, config)
            new_value = value_mapping[new_value]

        # Apply regex-based transformations
        for regex, replacement in regex_mapping:
            if re.search(regex, new_value):
                debug(f"ğŸ” Regex '{regex}' matched. Replacing '{new_value}' with '{replacement}'", LOG_LEVEL_ACTION, config)
                new_value = re.sub(regex, replacement, new_value)
                break

        return key, new_value
    elif action_type == "append_after":
        content = action.get("content", "")
        debug(f"â• Appending '{content}' to value '{value.strip()}'", LOG_LEVEL_ACTION, config)
        return key, f"{value.strip()}{content}"
    else:
        debug(f"âš ï¸ Unsupported action type '{action_type}' for key '{key}'", LOG_LEVEL_ERROR, config)
        return key, value

def transform_metadata(lines: List[str], config: Dict[str, Any]) -> List[str]:
    """
    æ ¹æ®è§„åˆ™è½¬æ¢å…ƒæ•°æ®è¡Œã€‚

    å‚æ•°:
        lines (list): å¯èƒ½åŒ…å«å…ƒæ•°æ®çš„è¡Œåˆ—è¡¨
        metadata_rules (dict): å¤„ç†å…ƒæ•°æ®çš„è§„åˆ™
        config (dict): é…ç½®å­—å…¸

    è¿”å›:
        list: è½¬æ¢åçš„è¡Œ
    """
    debug("Starting metadata transformation...", LOG_LEVEL_DEBUG, config)
    transformed_lines = []
    metadata_rules = config.get("metadata_rules", {})
    for line in lines:
        if line.strip().startswith('#'):
            continue
        key, sep, value = line.partition(": ")
        if not sep:  # Skip lines that are not metadata
            transformed_lines.append(line)
            continue

        debug(f"ğŸ” Processing metadata: {key}: {value.strip()}", LOG_LEVEL_DEBUG, config)
        rule = metadata_rules.get(key)
        if not rule:  # No rule, retain the line
            debug(f"âš ï¸ No rule found for key: {key}", LOG_LEVEL_ERROR, config)
            transformed_lines.append(line)
            continue

        actions = rule.get("actions", [])
        debug(f"ğŸ”§ Found {len(actions)} actions for key: {key}", LOG_LEVEL_DEBUG, config)

        current_key, current_value = key, value.strip()
        for i, action in enumerate(actions):
            debug(f"ğŸ”§ Applying action {i + 1}: {action.get('type')}", LOG_LEVEL_DEBUG, config)
            current_key, current_value = apply_action(current_key, current_value, action, config)
            if current_key is None:  # If deleted, stop processing further actions
                debug(f"ğŸ—‘ï¸ Key deleted: {key}", LOG_LEVEL_ACTION, config)
                break

        if current_key is not None:  # If not deleted, add the transformed metadata
            transformed_line = f"{current_key}: {current_value}"
            debug(f"âœ… Transformed: {transformed_line}", LOG_LEVEL_ACTION, config)
            transformed_lines.append(transformed_line)

    return transformed_lines

def update_references_in_markdown(file: Union[str, Path], path_mapping: Dict[str, str], config: Dict[str, Any]) -> None:
    """
    æ›´æ–° Markdown æ–‡ä»¶ä¸­çš„å¼•ç”¨å¹¶è½¬æ¢å…ƒæ•°æ®ã€‚

    å‚æ•°:
        file (stræˆ–Path): Markdown æ–‡ä»¶çš„è·¯å¾„ã€‚
        path_mapping (dict): å½“å‰æ–‡ä»¶çš„æ—§è·¯å¾„åˆ°æ–°è·¯å¾„çš„æ˜ å°„ã€‚
        config (dict, optional): ç”¨äºæ—¥å¿—è®°å½•çš„é…ç½®ã€‚
    """
    file_handle = safe_open_file(file, "r")
    if not file_handle:
        return

    with file_handle:
        content = file_handle.readlines()

    debug(f"Updating references and metadata in: {file}", LOG_LEVEL_DEBUG, config)
    debug(f"Path mapping: {path_mapping}", LOG_LEVEL_DEBUG, config)

    updated_content = []
    for i, line in enumerate(content, start=1):
        original_line = line
        for old_path, new_path in path_mapping.items():
            if old_path in line:
                encoded_new_path = quote(new_path)
                line = line.replace(old_path, encoded_new_path)
                debug(f"Line {i}:\n   Original: {original_line.strip()}\n   Updated:  {line.strip()}", LOG_LEVEL_DEBUG, config)
        updated_content.append(line)

    file_handle = safe_open_file(file, "w")
    if file_handle:
        with file_handle:
            file_handle.writelines(updated_content)

    debug(f"æ›´æ–°æ–‡ä»¶å¼•ç”¨å®Œæˆ {file}", LOG_LEVEL_ACTION, config)

def map_metadata(file: Union[str, Path], config: Dict[str, Any]) -> None:
    """
    æ˜ å°„å¹¶è½¬æ¢ Markdown æ–‡ä»¶ä¸­çš„å…ƒæ•°æ®ã€‚

    å‚æ•°:
        file (str æˆ– Path): Markdown æ–‡ä»¶çš„è·¯å¾„
        config (dict): é…ç½®å­—å…¸
    """
    metadata_rules = config.get("metadata_rules", {})
    file_handle = safe_open_file(file, "r")
    if not file_handle:
        return

    with file_handle:
        content = file_handle.readlines()

    metadata_end_index = 0
    for i, line in enumerate(content):
        if line.strip() == "":  # Assume metadata ends at the first blank line
            metadata_end_index = i
            break

    metadata_lines = content[:metadata_end_index]
    transformed_metadata = transform_metadata(metadata_lines, config)

    debug(f"Metadata changes for {file}:", LOG_LEVEL_DEBUG, config)
    for original, transformed in zip(metadata_lines, transformed_metadata):
        if original.strip() != transformed.strip():
            debug(f"   Original: {original.strip()}\n   Transformed: {transformed.strip()}", LOG_LEVEL_DEBUG, config)

    content = transformed_metadata + content[metadata_end_index:]

    file_handle = safe_open_file(file, "w")
    if file_handle:
        with file_handle:
            file_handle.writelines(content)

    debug(f"æ˜ å°„æ–‡ä»¶ {file} çš„å…ƒæ•°æ®å®Œæˆ", LOG_LEVEL_ACTION, config)

#############################################################
# MAIN FUNCTION
#############################################################

# Functions in this section handle the main script execution.
# These include `print_introduction`, `parse_arguments`, `load_and_configure`,
# `print_statistics`, `confirm_execution`, `print_final_statistics`, and `main`.

def print_introduction() -> None:
    """
    æ‰“å°è„šæœ¬çš„ç®€è¦ä»‹ç»ã€‚
    """
    intro = """
ğŸ’¼ Obsidian Import Tool ğŸ“š
--------------------------
A tool to process Notion exports for use in Obsidian.

This tool helps you:
- Clean up filenames by removing UIDs
- Consolidate all attachments into a single resource directory
- Update markdown references to point to the new paths
- Transform metadata based on custom rules

Usage examples:
  python3 obsidian_import.py /path/to/notion/export  # Process an export directory
  python3 obsidian_import.py --help                  # Show all available options

Get started by running with a directory path.
"""
    print(intro)

def parse_arguments() -> argparse.ArgumentParser:
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚
    """
    parser = argparse.ArgumentParser(
        description="Transform Notion exports for use in Obsidian - consolidates resources and cleans up metadata.",
        epilog="For detailed documentation, refer to the script comments."
    )
    parser.add_argument("directory", help="The directory to process", nargs="?")
    parser.add_argument("--config", help="Path to the configuration file", default="obsidian_import.yaml")
    parser.add_argument("--log", nargs="?", const=LOG_LEVEL_ACTION, choices=LOG_LEVELS.keys(), type=str.upper,
                        help="Enable logging to a file with an optional log level (default: ACT if no level is provided)")
    parser.add_argument("--verbose", nargs="?", const=LOG_LEVEL_ACTION, choices=LOG_LEVELS.keys(), type=str.upper,
                        help="Enable verbose output with an optional stdout level (default: ACT if no level is provided)")
    parser.add_argument("--reset-log", action="store_true", help="Clear the log file before starting")
    return parser

def load_and_configure(args: argparse.Namespace) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶åŠ è½½é…ç½®å¹¶åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ã€‚

    å‚æ•°:
        args (argparse.Namespace): è§£æçš„å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        dict: åº”ç”¨è¦†ç›–åçš„é…ç½®å­—å…¸
    """
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if args.config and not os.path.isfile(args.config):
        debug(f"Error: Configuration file '{args.config}' does not exist.", LOG_LEVEL_ERROR, {})
        sys.exit(1)

    # å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
    config = load_config(args.config)
    if not config:
        debug(f"Error: Failed to load configuration file '{args.config}'.", LOG_LEVEL_ERROR, {})
        sys.exit(1)

    # Apply command-line overrides
    config["log_file"] = "obsidian_import.log" if args.log else None
    config["log_level"] = args.log if args.log else LOG_LEVEL_ACTION
    config["stdout_level"] = args.verbose if args.verbose else LOG_LEVEL_FLOW
    config["reset_log"] = args.reset_log

    return config

def print_statistics(config: Dict[str, Any], tasks: List[Dict[str, Any]]) -> None:
    """
    Print task summary statistics in a tree structure.

    Parameters:
        config (dict): Configuration dictionary
        tasks (list): Tasks generated during scanning
    """
    stats = config["stats"]
    unmapped_metadata = stats.get("unmapped_metadata", {})
    
    # If there's a progress bar, make sure it's completed before printing statistics
    if hasattr(display_progress_bar, "last_line"):
        sys.stdout.write("\n")
        sys.stdout.flush()
    
    # Count preprocessing tasks
    pre_tasks = [task for task in tasks if task.get("is_pre_task", False) and task.get("status") == "todo"]

    print("\nğŸ“Š Import Statistics")
    print("â”œâ”€ ğŸ“ Summary")
    print(f"â”‚  â”œâ”€ Processed Markdown files    : {stats.get('markdown_files', 0)}")
    print(f"â”‚  â”œâ”€ Processed attachments       : {stats.get('attachments', 0)}")
    print(f"â”‚  â”œâ”€ Metadata conversion tasks   : {stats.get('metadata_tasks', 0)}")
    print(f"â”‚  â”œâ”€ Preprocessing tasks         : {len(pre_tasks)}")
    print(f"â”‚  â”œâ”€ Unmapped metadata entries   : {len(unmapped_metadata)}")
    print(f"â”‚  â”œâ”€ Scanning phase errors       : {stats.get('errors', 0)}")
    print(f"â”‚  â””â”€ Total tasks generated       : {len(tasks)}")

    print("â”œâ”€ ğŸ” Details")
    task_counts = {}
    for task in tasks:
        task_type = task['type']
        if task_type not in task_counts:
            task_counts[task_type] = 0
        task_counts[task_type] += 1

    task_types = list(task_counts.keys())
    for i, task_type in enumerate(task_types):
        prefix = "â”‚  â””â”€" if i == len(task_types) - 1 else "â”‚  â”œâ”€"
        print(f"{prefix} {task_type:28}: {task_counts[task_type]} tasks")

    # Only print preprocessing tasks when they exist
    if pre_tasks:
        print("â””â”€ ğŸ”„ Preprocessing Tasks")
        # Count INSERT_CONTENT tasks
        insert_content_tasks = [t for t in pre_tasks if t.get('type') == "INSERT_CONTENT"]
        other_pre_tasks = [t for t in pre_tasks if t.get('type') != "INSERT_CONTENT"]
        
        # Display count of INSERT_CONTENT tasks if there are any
        if insert_content_tasks:
            prefix = "   â”œâ”€" if other_pre_tasks else "   â””â”€"
            print(f"{prefix} INSERT_CONTENT              : {len(insert_content_tasks)} tasks")
        
        # Display other preprocessing tasks individually
        for i, task in enumerate(other_pre_tasks):
            is_last = i == len(other_pre_tasks) - 1
            prefix = "   â””â”€" if is_last else "   â”œâ”€"
            task_src = task.get('src', task.get('file', 'N/A'))
            if isinstance(task_src, Path):
                task_src = task_src.name
            print(f"{prefix} {task['type']:28}: {task_src}")
    else:
        print("â””â”€ ğŸ”„ Preprocessing Tasks:")
        print("   â””â”€ None")

    # If there are unmapped metadata entries, add a branch in the tree
    if unmapped_metadata:
        print("\nâš ï¸  Unmapped Metadata")
        metadata_keys = list(unmapped_metadata.keys())
        
        for i, key in enumerate(metadata_keys):
            values = unmapped_metadata[key]
            is_last_key = i == len(metadata_keys) - 1
            key_prefix = "â””â”€" if is_last_key else "â”œâ”€"
            print(f"{key_prefix} {key:28}: {len(values)} values")
            
            for j, value in enumerate(values):
                value_prefix = "   â””â”€" if j == len(values) - 1 else "   â”œâ”€"
                print(f"{' ' if is_last_key else 'â”‚'}{value_prefix} {value}")

def confirm_execution() -> bool:
    """
    åœ¨æ‰§è¡Œä»»åŠ¡åˆ—è¡¨ä¹‹å‰æç¤ºç”¨æˆ·ç¡®è®¤ã€‚

    è¿”å›:
        bool: å¦‚æœç”¨æˆ·ç¡®è®¤åˆ™ä¸º Trueï¼Œå¦åˆ™ä¸º False
    """
    response = input("\nDo you want to proceed with the tasks? (yes/no): ").strip().lower()
    return response in ["yes", "y"]

def print_final_statistics(tasks: List[Dict[str, Any]], execution_time: float, config: Dict[str, Any]) -> None:
    """
    åœ¨æ‰§è¡Œä»»åŠ¡åæ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯ã€‚

    å‚æ•°:
        tasks (list): å·²æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨
        execution_time (float): æ€»æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
        config (dict): é…ç½®å­—å…¸
    """
    print("\nâœ… Task Execution Complete")
    print("--------------------------")
    print(f"Total tasks executed: {len(tasks)}")
    print(f"Execution time: {execution_time:.2f} seconds")

    if config.get("log_debug", False):
        debug(f"Task execution completed in {execution_time:.2f} seconds", LOG_LEVEL_ACTION, config)

def main() -> None:
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºåè°ƒ Obsidian å¯¼å…¥è¿‡ç¨‹ã€‚
    """
    parser = parse_arguments()
    if len(sys.argv) == 1:
        print_introduction()
        parser.print_help()
        return

    args = parser.parse_args()

    if not args.directory:
        parser.error("the following arguments are required: directory")

    config = load_and_configure(args)
    
    # Initialize the log file if requested
    initialize_log_file(config)
    
    debug("ğŸš€ Starting Obsidian Import Tool...", LOG_LEVEL_FLOW, config)

    directory = args.directory
    attachment_output_path = config.get("attachment_output_path", "Resource")

    if config.get("log_file"):
        debug(f"Starting obsidian_import.py on {directory}", LOG_LEVEL_ACTION, config)

    # Step 1: Scan the directory
    debug("ğŸš€ Starting directory scan...", LOG_LEVEL_FLOW, config)
    tasks = scan_directory(directory, attachment_output_path, config)

    # Step 2: Confirm execution
    print_statistics(config, tasks)
    if not confirm_execution():
        debug("Execution cancelled by user.", LOG_LEVEL_FLOW, config)
        return
    
    # Step 3: Execute tasks
    debug("ğŸš€ Executing tasks...", LOG_LEVEL_FLOW, config)
    execute_tasks(tasks, config)

    # Step 4: Print final statistics
    print_statistics(config, tasks)

    close_log_file(config)

if __name__ == "__main__":
    main()
